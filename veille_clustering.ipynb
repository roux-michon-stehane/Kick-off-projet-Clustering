{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a21b5a",
   "metadata": {},
   "source": [
    "# Veille sur les algorithmes de clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b6db8",
   "metadata": {},
   "source": [
    "Dans le clustering, l'objectif est de regrouper les données dans des groupes distincts sur la base des données.\n",
    "Le clustering est une technique d'apprentissage automatique qui implique le regroupement de points de données. Étant donné un ensemble de points de données, nous pouvons utiliser un algorithme de clustering pour classer chaque point de données dans un groupe spécifique. En théorie, les points de données qui sont dans le même groupe devraient avoir des propriétés et/ou des caractéristiques similaires, tandis que les points de données dans des groupes différents devraient avoir des propriétés et/ou des caractéristiques très différentes.\n",
    "\n",
    "En science des données, nous pouvons utiliser l'analyse de clustering pour obtenir des informations précieuses sur nos données en observant les groupes dans lesquels les points de données se classent lorsque nous appliquons la méthode de clustering.\n",
    "\n",
    "Parmi les applications du clustering, citons : la segmentation d'images, le regroupement de documents, la détection d'anomalies et les moteurs de recommandation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d39b6",
   "metadata": {},
   "source": [
    "1-  Algorithme K-means "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe244f",
   "metadata": {},
   "source": [
    "L'algorithme K-means, également connu sous le nom de \"partitionnement en K-moyennes\", est un algorithme de clustering non supervisé utilisé pour partitionner un ensemble de données en k clusters distincts. L'algorithme repose sur la définition d'un certain nombre de centres de clusters, qui sont généralement choisis au hasard au début de l'algorithme. Ensuite, chaque point de donnée est assigné au centre de cluster le plus proche, puis les centres de cluster sont mis à jour en calculant la nouvelle moyenne des points de données assignés à chaque cluster. Ce processus de réassignation et de mise à jour des centres de cluster est répété jusqu'à ce que les centres de cluster convergent vers une solution stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcb9d9",
   "metadata": {},
   "source": [
    "2- DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b48de",
   "metadata": {},
   "source": [
    "DBSCAN est un algorithme de regroupement basé sur la densité.\n",
    "Les étapes de l'algorithme : \n",
    "1- DBSCAN commence par un point de départ arbitraire qui n'a pas été visité. Le voisinage de ce point est extrait à l'aide d'une distance\n",
    "epsilon ε (tous les points situés à l'intérieur de la distance ε sont des points de voisinage). \n",
    "2- S'il y a un nombre suffisant de points (selon minPoints) dans ce voisinage, le processus de regroupement commence et le point de données actuel devient le premier point.\n",
    "le point de données actuel devient le premier point du nouveau cluster.\n",
    "Dans le cas contraire, le point sera étiqueté comme bruit (plus tard, ce point bruyant pourra faire partie de la grappe). Dans les deux cas, ce point\n",
    "est marqué comme \"visité\".\n",
    "3- Pour ce premier point de la nouvelle grappe, les points situés dans son voisinage de distance ε font également partie de la même grappe.\n",
    "Cette procédure d'appartenance de tous les points du voisinage ε à la même grappe est ensuite répétée pour tous les nouveaux points qui viennent d'être ajoutés au groupe de grappes.\n",
    "qui viennent d'être ajoutés au groupe de clusters. \n",
    "4- Ce processus des étapes 2 et 3 est répété jusqu'à ce que tous les points de la grappe soient déterminés, c'est-à-dire tous les points\n",
    " dans le voisinage ε de la grappe ont été visités et étiquetés.\n",
    "5- Une fois que nous en avons terminé avec le groupe actuel, un nouveau point non visité est récupéré et traité, ce qui conduit à la découverte d'un autre groupe\n",
    "ou d'un bruit. Ce processus se répète jusqu'à ce que tous les points soient marqués comme visités. Étant donné qu'à la fin de ce processus, tous les points ont été visités, chaque point aura été marqué comme appartenant à une grappe ou à un bruit.\n",
    "aura été marqué comme appartenant à une grappe ou comme étant un bruit.\n",
    "\n",
    "DBSCAN présente de grands avantages par rapport à d'autres algorithmes de regroupement. Tout d'abord, il ne nécessite pas du tout un nombre pair de grappes.\n",
    "Il identifie également les valeurs aberrantes comme des bruits.\n",
    "Le principal inconvénient de DBSCAN est qu'il n'est pas aussi performant que les autres algorithmes lorsque les grappes sont de densité variable. Cela s'explique par le fait que le\n",
    "seuil de distance ε et minPoints pour l'identification des points de voisinage varie d'un cluster à l'autre lorsque la densité varie.\n",
    "densité varie. Cet inconvénient se produit également avec des données de très haute dimension car, là encore, le seuil de distance ε devient difficile à estim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041567e",
   "metadata": {},
   "source": [
    "3- CAH : Classification Ascendante Hierarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a5c76",
   "metadata": {},
   "source": [
    "Il existe deux catégories : \n",
    "Les algorithmes ascendants traitent chaque point de données comme une seule grappe au départ, puis fusionnent (ou agglomèrent) successivement des paires de grappes jusqu'à ce que toutes les grappes aient été fusionnées en une seule grappe qui contient tous les points de données. \n",
    "paires de grappes jusqu'à ce que toutes les grappes aient été fusionnées en une seule grappe qui contient tous les points de données. La classification hiérarchique ascendante\n",
    "ascendante est donc appelée clustering agglomératif hiérarchique ou HAC. Cette hiérarchie de grappes est représentée sous la forme d'un arbre (ou dendrogramme).\n",
    "Les étapes de l'algorithme :\n",
    "\n",
    "1- Nous commençons par traiter chaque point de données comme un seul cluster, c'est-à-dire que s'il y a X points de données dans notre ensemble de données, nous avons X clusters.\n",
    " Nous sélectionnons ensuite une métrique de distance qui mesure la distance entre deux clusters. À titre d'exemple, nous utiliserons le lien moyen qui définit la distance entre deux clusters comme la distance moyenne entre les points de données du premier cluster et les points de données du second cluster.\n",
    " \n",
    "2- À chaque itération, nous combinons deux grappes en une seule. Les deux grappes à combiner sont sélectionnées comme étant celles qui ont le plus petit lien moyen.plus petit lien moyen. En d'autres termes, selon la métrique de distance que nous avons choisie, ces deux grappes ont la plus petite distance entre elles et sont donc les plus similaires et doivent être combinés.\n",
    "\n",
    "3- L'étape 2 est répétée jusqu'à ce que nous atteignions la racine de l'arbre, c'est-à-dire jusqu'à ce que nous n'ayons plus qu'un seul groupe contenant tous les points de données. De cette manière nous pouvons choisir le nombre de grappes que nous voulons au final, simplement en choisissant le moment où nous arrêtons de combiner les grappes, c'est-à-dire le moment où nous arrêtons de construire l'arbre !\n",
    "\n",
    "Le regroupement hiérarchique ne nécessite pas de spécifier le nombre de grappes et nous pouvons même choisir le nombre de grappes qui nous semble le plus approprié, puisque nous construisons un arbre.\n",
    "puisque nous construisons un arbre. En outre, l'algorithme n'est pas sensible au choix de la métrique de distance.\n",
    "alors qu'avec d'autres algorithmes de clustering, le choix de la métrique de distance est critique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28cc0fe",
   "metadata": {},
   "source": [
    "### Réalisez une veille sur les méthodes de sélection du nombre clusters optimal et la mesure de qualité d’un cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318cca3",
   "metadata": {},
   "source": [
    "METHODES DE SELECTION DU NOMBRE CLUSTERS OPTIMAL  :\n",
    "- Elbow method ou ( méthode du coude ) : . Elle s’appuie sur la notion d’inertie. On définit cette dernière comme ceci : la somme des distances euclidiennes entre chaque point et son centroïde associé. Evidemment plus on fixe un nombre initial de clusters élevés et plus on réduit l’inertie : les points ont plus de chance d’être à côté d’un centroïde.\n",
    "- le coefficient de silhouette : le coefficient de silhouette est défini pour chaque échantillon et se compose de deux scores:\n",
    "\n",
    "\ta : La distance moyenne entre un échantillon et tous les autres points de la même classe.\n",
    "\tb : La distance moyenne entre un échantillon et tous les autres points de la grappe suivante la plus proche .\n",
    "                  \n",
    "Un score de coefficient de silhouette plus élevé est lié à un modèle avec des clusters mieux définis .\n",
    "Ainsi, la meilleure valeur est 1 et la pire valeur est -1. Les valeurs proches de 0 indiquent des clusters qui se chevauchent. Les valeurs négatives indiquent généralement qu'un échantillon a été affecté au mauvais cluster, car un autre cluster est plus similaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97cc32",
   "metadata": {},
   "source": [
    "MESURE DE QUALITE D'UN CLUSTER :\n",
    "\n",
    "-Score d'homogénéité : le résultat d'un cluster est dit homogène si ses clusters ne contiennent que des données appartenant à une seule classe.\n",
    "\n",
    "-Score de complétude : ce score vérifie que tous les membres d'une certaine classe sont attribués au même cluster.\n",
    "\n",
    "-Score de mesure V. Il s'agit de la moyenne harmonique entre l'homogénéité et l'exhaustivité. \n",
    "\n",
    "-Score Rand ajusté. Il s'agit de l'indice de Rand ajusté pour tenir compte du hasard. Il calcule la mesure de similarité de deux regroupements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cd66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
